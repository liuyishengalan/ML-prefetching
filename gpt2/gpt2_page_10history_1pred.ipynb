{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "from itertools import islice\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "buffer = []\n",
    "numrows = 1000000\n",
    "# numrows = 10000\n",
    "with open('../page.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in islice(reader, numrows):\n",
    "        buffer.append({\n",
    "            \"pc\": row[\"pc\"],\n",
    "            \"page_in\": row[\"page_in\"],\n",
    "            \"page_out\": row[\"page_out\"]\n",
    "        })\n",
    "        if len(buffer) == 10:\n",
    "            data.append(buffer)\n",
    "            buffer = []\n",
    "if buffer:  # handle any remaining rows\n",
    "    data.append(buffer)\n",
    "\n",
    "split_index = int(len(data) * 0.2)\n",
    "train_data = data[:split_index]\n",
    "validation_data = data[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryAccessDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data):\n",
    "        \"\"\"\n",
    "        Initializes the MemoryAccessDataset.\n",
    "\n",
    "        Args:\n",
    "        - tokenizer: The tokenizer used to encode the sequences.\n",
    "        - data: The data to be processed, expected to be a list of chunks, where each chunk\n",
    "                contains multiple traces of memory access data.\n",
    "        \"\"\"\n",
    "        self.input_ids = []\n",
    "        self.attention_masks = []\n",
    "        self.labels = []\n",
    "        for chunk in data:\n",
    "            sequences = []\n",
    "            labels = []\n",
    "            # For each trace in the chunk, create sequences and labels\n",
    "            for trace in chunk:\n",
    "                sequences.append(f\"PC: {trace['pc']} Page: {trace['page_in']}\")\n",
    "                labels.append(trace['page_out'])\n",
    "            \n",
    "            # Concatenate all sequences in the chunk\n",
    "            full_sequence = \" \".join(sequences)\n",
    "            encoding = tokenizer(full_sequence, max_length=512, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "            label_encoding = tokenizer(\" \".join(labels), max_length=512, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "            \n",
    "            # Append the encoded inputs and attention masks to the respective lists\n",
    "            self.input_ids.append(encoding.input_ids.squeeze())\n",
    "            self.attention_masks.append(encoding.attention_mask.squeeze())\n",
    "            \n",
    "            # Append the encoded labels to the labels list\n",
    "            self.labels.append(label_encoding.input_ids.squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the input_ids, attention_mask, and labels for a given index.\n",
    "\n",
    "        Returns:\n",
    "        A tuple containing:\n",
    "        - input_ids: The tokenized input sequence.\n",
    "        - attention_mask: The attention mask for the input sequence.\n",
    "        - labels: The tokenized labels.\n",
    "        \"\"\"\n",
    "        return self.input_ids[idx], self.attention_masks[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Prepare datasets and dataloaders\n",
    "train_dataset = MemoryAccessDataset(tokenizer, train_data)\n",
    "validation_dataset = MemoryAccessDataset(tokenizer, validation_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=2)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "loss_values = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, masks, labels = batch\n",
    "        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch + 1}, Average training loss: {avg_train_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_page_address(model, tokenizer, sequence):\n",
    "    \"\"\"\n",
    "    Predicts the next page address given a sequence of program counters and page addresses.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer.encode(sequence, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, \n",
    "                                 max_length=inputs.shape[1] + 20, \n",
    "                                 num_return_sequences=1, \n",
    "                                 pad_token_id=tokenizer.eos_token_id)\n",
    "    predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Assume the last word is the next page\n",
    "    predicted_page_address = predicted_text.split()[-1] \n",
    "    return predicted_page_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, tokenizer, data):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    # Iterate through each chunk in the validation data\n",
    "    for chunk in data:\n",
    "        # Create the input sequence by concatenating program counters and page addresses, except the last one\n",
    "        sequence = ' '.join([f\"PC: {trace['pc']} Page: {trace['page_in']}\" for trace in chunk[:-1]])\n",
    "        actual_next_page = chunk[-1]['page_out']\n",
    "        predicted_page_address = predict_next_page_address(model, tokenizer, sequence)\n",
    "        \n",
    "        if predicted_page_address.strip() == actual_next_page.strip():\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "        print(f\"Predicted: {predicted_page_address}, Actual: {actual_next_page}\")\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "accuracy = test_accuracy(model, tokenizer, validation_data)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# model_path = \"/home/yisheng/data/gpt2_trained_model\"\n",
    "# model.save_pretrained(model_path)\n",
    "\n",
    "# # Save the tokenizer\n",
    "# tokenizer_path = \"/home/yisheng/data/gpt2_trained_tokenizer\"\n",
    "# tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# # Load the model\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# # Ensure the model is in evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# # Example usage\n",
    "# pc = \"140203333008656\"  # Example PC value\n",
    "# page_address = \"-77547\"  # Example current page address\n",
    "# predicted_page_address = predict_next_page_address(model, tokenizer, pc, page_address)\n",
    "# print(f\"Predicted Next Page Address: {predicted_page_address}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
